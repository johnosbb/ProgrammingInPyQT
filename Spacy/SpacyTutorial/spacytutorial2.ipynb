{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America. It consists of 50 states, a federal district, five major unincorporated territories, 326 Indian reservations, and some minor possessions.[j] At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area.[d] The United States shares significant land borders with Canada to the north and Mexico to the south, as well as limited maritime borders with the Bahamas, Cuba, and Russia.[22] With a population of more than 331 million people, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York.\n",
      "\n",
      "Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775–1783), which established independence. In the late 18th century, the U.S. began expanding across North America, gradually obtaining new territories, sometimes through war, frequently displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent. Slavery was legal in the southern United States until the second half of the 19th century when the American Civil War led to its abolition. The Spanish–American War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II.\n",
      "\n",
      "During the Cold War, the United States fought the Korean War and the Vietnam War but avoided direct military conflict with the Soviet Union. The two superpowers competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon. The Soviet Union's dissolution in 1991 ended the Cold War, leaving the United States as the world's sole superpower.\n",
      "\n",
      "The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States, NATO, and other international organizations. It is a permanent member of the United Nations Security Council. Considered a melting pot of cultures and ethnicities, its population has been profoundly shaped by centuries of immigration. The country ranks high in international measures of economic freedom, quality of life, education, and human rights, and has low levels of perceived corruption. However, the country has received criticism concerning inequality related to race, wealth and income, the use of capital punishment, high incarceration rates, and lack of universal health care.\n",
      "\n",
      "The United States is a highly developed country, accounts for approximately a quarter of global GDP, and is the world's largest economy. By value, the United States is the world's largest importer and the second-largest exporter of goods. Although its population is only 4.2% of the world's total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Making up more than a third of global military spending, it is the foremost military power in the world; and it is a leading political, cultural, and scientific force internationally.[23]\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/wiki_us.txt\", \"r\" ) as file:\n",
    "    text = file.read()\n",
    "print(text)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The United States of America (U.S.A. or USA), commonly known as the United States (U.S. or US) or America, is a country primarily located in North America.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "sentence1 = list(doc.sents)[0]\n",
    "print(sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country—0,467', 'nationâ\\x80\\x99s', 'countries-', 'continente', 'Carnations', 'pastille', 'бесплатно', 'Argents', 'Tywysogion', 'Teeters']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "your_word = \"country\"\n",
    "ms = nlp.vocab.vectors.most_similar(np.asarray([nlp.vocab.vectors[nlp.vocab.strings[your_word]]]), n=10)\n",
    "words = [nlp.vocab.strings[w] for w in ms[0][0]]\n",
    "distances = ms[2]\n",
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'summary': {'sentencizer': {'assigns': ['token.is_sent_start', 'doc.sents'],\n",
       "   'requires': [],\n",
       "   'scores': ['sents_f', 'sents_p', 'sents_r'],\n",
       "   'retokenizes': False}},\n",
       " 'problems': {'sentencizer': []},\n",
       " 'attrs': {'doc.sents': {'assigns': ['sentencizer'], 'requires': []},\n",
       "  'token.is_sent_start': {'assigns': ['sentencizer'], 'requires': []}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "# if you do not always need every aspect of the pipeline then exclude it, simply load what you need.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "s = requests.get(\"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\")\n",
    "soup = BeautifulSoup(s.content).text.replace(\"-\\n\", \"\").replace(\"\\n\", \" \")\n",
    "nlp.max_length = 5278439\n",
    "doc = nlp(soup)\n",
    "print (len(list(doc.sents)))\n",
    "nlp.analyze_pipes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Deeds PERSON\n",
      "West Chestertenfieldville GPE\n",
      "Deeds PERSON\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"The village of Treblinka is in Poland. Treblinka was also an extermination camp.\"\n",
    "text = \"West Chestertenfieldville was referenced in Mr. Deeds.\"\n",
    "#Create the Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)\n",
    "    \n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "nlp.analyze_pipes() \n",
    "\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"Treblinka\"}\n",
    "            ]\n",
    "\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"West Chestertenfieldville\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "West Chestertenfieldville GPE\n",
      "Deeds PERSON\n",
      "West Chestertenfieldville GPE\n",
      "Mr. Deeds Film\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Sample text\n",
    "text = \"West Chestertenfieldville was referenced in Mr. Deeds.\"\n",
    "#Create the Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)\n",
    "    \n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "\n",
    "\n",
    "patterns = [\n",
    "                {\"label\": \"GPE\", \"pattern\": \"West Chestertenfieldville\"},\n",
    "                {\"label\": \"Film\", \"pattern\": \"Mr. Deeds\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(16571425990740197027, 5, 6)]\n",
      "EMAIL_ADDRESS\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"LIKE_EMAIL\" : True}]\n",
    "matcher.add(\"EMAIL_ADDRESS\", [pattern])\n",
    "doc = nlp(\"This an email address: johnos3738@gmail.com\")\n",
    "matches = matcher(doc)\n",
    "print(matches)\n",
    "print(nlp.vocab[matches[0][0]].text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Wikimedia Foundation, Inc. (WMF), is an American 501(c)(3) nonprofit organization headquartered in San Francisco, California, and registered as a charitable foundation under local laws. Best known as the hosting platform for Wikipedia, a crowdsourced online encyclopedia, it also hosts other related projects and MediaWiki, a wiki software.The Wikimedia Foundation was established in 2003 in St. Petersburg, Florida, by Jimmy Wales as a nonprofit way to fund Wikipedia, Wiktionary, and other crowdsourced wiki projects that had until then been hosted by Bomis, Wales's for-profit company. The Foundation finances itself mainly through millions of small donations from Wikipedia readers, collected through email campaigns and annual fundraising banners placed on Wikipedia and its sister projects. These are complemented by grants from philanthropic organizations and tech companies, and starting in 2022, by services income from Wikimedia Enterprise.\n",
      "The Foundation has grown rapidly throughout its existence. By 2022, it employed around 700 staff and contractors, with annual revenues of US$155 million, annual expenses of US$146 million, net assets of US$240 million and a growing endowment, which surpassed US$100 million in June 2021.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Martin Luther King Jr.',\n",
       " 'Martin Luther King Jr. Day',\n",
       " 'Assassination of Martin Luther King Jr.',\n",
       " 'Martin Luther King III',\n",
       " 'Martin Luther King Sr.',\n",
       " 'Martin Luther King Jr. Memorial',\n",
       " 'Martin Luther King Jr. National Historical Park',\n",
       " 'FBI–King suicide letter',\n",
       " 'Martin Luther King Jr. assassination conspiracy theories',\n",
       " 'List of streets named after Martin Luther King Jr.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wikipedia\n",
    "print (wikipedia.summary(\"Wikipedia\"))\n",
    "wikipedia.search(\"Martin Luther King\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273\n",
      "(451313080118390996, 72, 74) King advanced\n",
      "(451313080118390996, 124, 126) King participated\n",
      "(451313080118390996, 318, 322) J. Edgar Hoover considered\n",
      "(451313080118390996, 363, 365) FBI mailed\n",
      "(451313080118390996, 389, 391) King won\n",
      "(451313080118390996, 530, 533) United States beginning\n",
      "(451313080118390996, 629, 631) King had\n",
      "(451313080118390996, 685, 687) Williams married\n",
      "(451313080118390996, 723, 727) King Sr. left\n",
      "(451313080118390996, 760, 762) Alberta began\n"
     ]
    }
   ],
   "source": [
    "with open (\"data/wiki_mlk.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "#print(text)    \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"POS\" : \"PROPN\", \"OP\" : \"+\" },{\"POS\": \"VERB\"}]\n",
    "matcher.add(\"PROPER_NOUN\", [pattern],greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "matches.sort(key = lambda x: x[1])\n",
    "print(len(matches))\n",
    "for match in matches[:10]:\n",
    "    print(match,doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was reading, but it had no pictures or conversations in it, 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "So she was considering in her own mind (as well as she could, for the hot day made her feel very sleepy and stupid), whether the pleasure of making a daisy-chain would be worth the trouble of getting up and picking the daisies, when suddenly a White Rabbit with pink eyes ran close by her.\n",
      "\n",
      "There was nothing so very remarkable in that; nor did Alice think it so very much out of the way to hear the Rabbit say to itself, 'Oh dear! Oh dear! I shall be late!' (when she thought it over afterwards, it occurred to her that she ought to have wondered at this, but at the time it all seemed quite natural); but when the Rabbit actually took a watch out of its waistcoat-pocket, and looked at it, and then hurried on, Alice started to her feet, for it flashed across her mind that she had never before seen a rabbit with either a waistcoat-pocket, or a watch to take out of it, and burning with curiosity, she ran across the field after it, and fortunately was just in time to see it pop down a large rabbit-hole under the hedge.\n",
      "1\n",
      "(451313080118390996, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n"
     ]
    }
   ],
   "source": [
    "with open (\"data/alice.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    text = text.replace(\"`\",\"'\")\n",
    "    print(text)\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    speak_lemmas = [\"think\", \"say\"]\n",
    "    pattern = [{\"ORTH\" : \"'\"},\n",
    "               {\"IS_ALPHA\" : True,\"OP\" : \"+\"},\n",
    "               {\"IS_PUNCT\" : True, \"OP\": \"*\"},\n",
    "               {\"ORTH\" : \"'\"},\n",
    "               {\"POS\" : \"VERB\", \"LEMMA\" : {\"IN\" : speak_lemmas}},\n",
    "               {\"POS\" : \"PROPN\", \"OP\" : \"+\"},\n",
    "               {\"ORTH\" : \"'\"},\n",
    "               {\"IS_ALPHA\" : True,\"OP\" : \"+\"},\n",
    "               {\"IS_PUNCT\" : True, \"OP\": \"*\"},\n",
    "               {\"ORTH\" : \"'\"}\n",
    "               ]\n",
    "    matcher.add(\"PROPER_NOUN\", [pattern],greedy=\"LONGEST\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    print(len(matches))\n",
    "    for match in matches[:10]:\n",
    "        print(match,doc[match[1]:match[2]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(3232560085755078826, 47, 67) 'and what is the use of a book,' thought Alice 'without pictures or conversation?'\n",
      "(3232560085755078826, 529, 535) 'Well!' thought Alice\n"
     ]
    }
   ],
   "source": [
    "with open (\"data/alice.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    speak_lemmas = [\"think\", \"say\"]\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    pattern1 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "    pattern2 = [{'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}, {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {\"POS\": \"PROPN\", \"OP\": \"+\"}]\n",
    "    pattern3 = [{\"POS\": \"PROPN\", \"OP\": \"+\"},{\"POS\": \"VERB\", \"LEMMA\": {\"IN\": speak_lemmas}}, {'ORTH': \"'\"}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': \"'\"}]\n",
    "    matcher.add(\"PROPER_NOUNS\", [pattern1, pattern2, pattern3], greedy='LONGEST')\n",
    "    text = text.replace(\"`\", \"'\")\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x: x[1])\n",
    "    print (len(matches))\n",
    "    for match in matches[:10]:\n",
    "        print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mary PERSON\n"
     ]
    }
   ],
   "source": [
    "from spacy import Language\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "\n",
    "@Language.component(\"remove_gpe\")\n",
    "def remove_gpe(doc):\n",
    "    original_ents = list(doc.ents)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\":\n",
    "            original_ents.remove(ent)\n",
    "    doc.ents = original_ents\n",
    "    return (doc) \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"remove_gpe\") \n",
    "doc = nlp(\"Britain is a place. Mary is a doctor.\")    \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_) \n",
    "nlp.to_disk(\"data/new_en_core_web_sm\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555-5555 PHONE_NUMBER\n"
     ]
    }
   ],
   "source": [
    "#Import the requisite library\n",
    "import spacy\n",
    "\n",
    "#Sample text\n",
    "text = \"This is a sample number 555-5555.\"\n",
    "\n",
    "#Build upon the spaCy Small Model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "#Create the Ruler and Add it\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns (source: https://spacy.io/usage/rule-based-matching)\n",
    "patterns = [\n",
    "                {\"label\": \"PHONE_NUMBER\", \"pattern\": [{\"SHAPE\": \"ddd\"},\n",
    "                {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]}\n",
    "            ]\n",
    "#add patterns to ruler\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "#create the doc\n",
    "doc = nlp(text)\n",
    "\n",
    "#extract entities\n",
    "for ent in doc.ents:\n",
    "    print (ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 11), match='James Joyce'>\n",
      "<re.Match object; span=(73, 87), match='James Stephens'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"James Joyce was an Irish writer, but Henry James was an American writer. James Stephens was another Irish writer.\"\n",
    "pattern = r\"James [A-Z]\\w+\" # find all expressions that begin with James followed by an word beginning with a capital letter and ending in a word break.\n",
    "matches = re.finditer(pattern,text)\n",
    "for match in matches:\n",
    "    print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "James Joyce\n",
      "James Stephens\n",
      "[(0, 2, 'James Joyce'), (15, 17, 'James Stephens')]\n",
      "(James Joyce, James Stephens)\n",
      "James Joyce PERSON\n",
      "James Stephens PERSON\n",
      "(James Joyce, James Stephens)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(text)\n",
    "original_ents = list(doc.ents)\n",
    "mwt_ents = []\n",
    "for match in re.finditer(pattern, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start,end)\n",
    "    print(span)\n",
    "    if span is not None:\n",
    "        mwt_ents.append((span.start, span.end, span.text))\n",
    "print(mwt_ents)\n",
    "for ent in mwt_ents:\n",
    "    start, end , name = ent\n",
    "    per_ent = Span(doc, start,end, label=\"PERSON\")\n",
    "    original_ents.append(per_ent)\n",
    "doc.ents = original_ents\n",
    "print(doc.ents)    \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "    \n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"james_ner\")\n",
    "def james_ner(doc):\n",
    "    pattern = r\"James [A-Z]\\w+\" # find all expressions that begin with James followed by an word beginning with a capital letter and ending in a word break.\n",
    "    original_ents = list(doc.ents)\n",
    "    mwt_ents = []\n",
    "    for match in re.finditer(pattern, doc.text):\n",
    "        start, end = match.span()\n",
    "        span = doc.char_span(start,end)\n",
    "        # print(span)\n",
    "        if span is not None:\n",
    "            mwt_ents.append((span.start, span.end, span.text))\n",
    "    # print(mwt_ents)\n",
    "    for ent in mwt_ents:\n",
    "        start, end , name = ent\n",
    "        per_ent = Span(doc, start,end, label=\"PERSON\")\n",
    "        original_ents.append(per_ent)\n",
    "    doc.ents = original_ents\n",
    "    return (doc)\n",
    "    # print(doc.ents)    \n",
    "    # for ent in doc.ents:\n",
    "    #     print(ent.text, ent.label_)\n",
    "    \n",
    "    \n",
    "nlp2 = spacy.blank(\"en\")\n",
    "nlp2.add_pipe(\"james_ner\")\n",
    "text = \"James Joyce was an Irish writer, but Henry James was an American writer. James Stephens was another Irish writer.\"    \n",
    "doc2 = nlp2(text)  \n",
    "print(doc2.ents)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
